{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最简单的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在介绍深度前馈网络之前，我们先介绍神经网络的基本单元——**神经元模型**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经元模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们采用如下关于**神经网络**的定义：\n",
    "\n",
    "    神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟神经系统对真实世界物体所作出的交互反应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural networks 中最基本的成分是 神经元(neuron)模型，即上述定义中的“简单单元”。模型借鉴如下生物神经网络的特性：\n",
    "\n",
    "    生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个“阈值”(threshold),那么它就会被激活，即“兴奋”起来，向其他神经元发送化学物质。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述特性被 McCulloch and Pitts 抽象为如下的简单模型，也就是一直沿用的**“M-P 神经元模型”**：\n",
    "\n",
    "    在这个模型中，神经元接收到来自 n 个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值与神经元的阈值进行比较，然后通过“激活”函数处理以产生神经元的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/mp-neuron.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经元模型的输出数学表达如下：\n",
    "\n",
    "$$y = f\\left(\\sum_{i=1}^n w_i x_i - \\theta\\right)$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $x_i$ 是第 i 个神经元的输入；\n",
    "* $w_i$ 是第 i 个神经元到该神经元的连接权重；\n",
    "* $\\theta$ 是该神经元的阈值；\n",
    "* f 是负责输出的“激活函数”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**激活函数 f**\n",
    "\n",
    "理想的激活函数是下图左侧的阶跃函数，将输入值映射为输出值 0 或 1,对应着神经元的“抑制”与“兴奋”。但是阶跃函数具有不连续、不光滑等不太好的性质，所以实际上常用 Sigmoid 函数作为激活函数，见下图右侧。Sigmoid 函数可以将较大范围内的输入挤压到(0,1)输出值范围内。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/act-fun.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机（Perceptron）由两层神经元组成，如下图所示：\n",
    "\n",
    "    输入层接收外界输入信号传递给输出层；输出层是M-P 神经元，也称为“阈值逻辑单元”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/perceptron.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**案例：与、或、非运算的实现**\n",
    "\n",
    "感知机可以实现逻辑与、或、非运算。假定 f 是阶跃函数。对应的三种运算为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|逻辑运算|参数$(w_1, w_2,\\theta)$|$\\quad$对应的数学模型 $y = f\\left(\\sum_{i=1}^n w_i x_i - \\theta\\right)$|$\\quad$实际运算$\\quad$|\n",
    "| :---: |:---:|:---:|:---:|\n",
    "|与：$x_1 \\land x_2$|(1,1,2)|$y=f(x_1+x_2-2)$|在 $x_1=x_2=1$时，y=1|\n",
    "|或：$x_1 \\lor x_2$|(1,1,0.5)|$y=f(x_1+x_2-0.5)$|在 $x_1=1$ 或 $x_2=1$ 时，y=1|\n",
    "|非：$\\urcorner x_1\\;\\quad$|(-0.6,0,-0.5)|$y=f(-0.6x_1-0.5)$|在 $x_1=0$时，y=1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更一般地，给定训练数据集，权重 $w_i \\;(i=1,2,\\cdots,n)$ 以及阈值 $\\theta$ 可以通过学习得到。阈值 $\\theta$ 可看做一个固定输入为 $-1.0$ 的“哑节点(dummy node)”所对应的连接权重 $w_{n+1}$，权重和阈值在这样的设定下可以统一为权重的学习。\n",
    "\n",
    "**学习规则**\n",
    "\n",
    "对训练样例 $(\\mathbf{x},y)$，假设当前感知机的输出为 $\\hat{y}$，则感知机权重调整方式为：\n",
    "\n",
    "$$w_i\\gets w_i + \\eta(y-\\hat{y})x_i$$\n",
    "\n",
    "其中 $\\eta$ 称为学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**感知机的局限性**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机只有输出层神经元进行激活函数处理，即**只拥有一层功能神经元**，学习能力非常有限。\n",
    "\n",
    "事实上，上面案例中的与或非问题都是线性可分问题。可以证明：若两类模式是线性可分的，即存在一个线性超平面能将它们分开，如下图的 (a)-(c) 所示，则感知机的学习过程一定会收敛而求得适当的权向量 $\\mathbf{w} = (w_1,w_2, \\cdots,w_{n+1})$；如果是非线性可分的，则感知机学习过程将会发生振荡， $\\mathbf{w}$ 难以稳定下来。\n",
    "\n",
    "例如感知机甚至不能解决下图所示的 (d) 所示的**异或**这样简单的非线性可分问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/linear.jpg' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要解决异或问题，需要引出我们今天的主题——**多层感知机，也就是深度前馈网络。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度前馈网络\n",
    "\n",
    "**深度前馈网络(deep feedforward network)**,也叫作 **前馈神经网络(feedforward neural network)** 或者 **多层感知机(multilayer perceptron,MLP)**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**多层感知机(MLP)定义**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **前向**：模型被称为**前向(feedforward)**的，是因为模型的输出和模型本身之间没有**反馈**连接。如果被扩展成反馈连接时，对应的被称为**循环神经网络**；\n",
    "\n",
    "\n",
    "* **网络**：前馈神经网络之所以被称作**网络**，是因为它通常用许多不同函数复合在一起所形成的复合映射：\n",
    "$$f(\\mathbf{x}) = f^{(n)} \\circ \\cdots \\circ f^{(2)}\\circ f^{(1)}(\\mathbf{x})$$\n",
    "从输入层开始，具有激活函数功能的 $f^{(k)}$ 称为第 k 层，其实质含义是**以第 k-1 层的输出作为输入，k-1 层到第 k 层之间的神经元连接权重所组成的权重矩阵，阈值向量，激活函数所一起构成的**。该模型对应的拓扑结构是一个有向无环图，如下图所示。\n",
    "\n",
    "\n",
    "* **隐藏层**：前馈神经网络的最后一层被称为**输出层**，介于输入层与输出层之间训练数据没有给出真实输出的层被称为**隐藏层**；\n",
    "\n",
    "\n",
    "* **深度**：我们定义一个前馈神经网络的**深度**为该网络所包含的**隐层层数**。正是因为“深度”这个术语，才出现了“深度学习”这个名字，神经网络要学习的就是不同层之间的连接权重以及阈值。\n",
    "\n",
    "\n",
    "**注**：“前馈”并不意味着网络中信号不能向后传播，不然BP算法也就不能用了，而是指网络拓扑结构上不存在环或者回路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/yinceng.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例：学习异或 XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR函数是两个二进制 $x_1,x_2$ 的运算。运算法则是：当这两个二进制值中恰好有一个为 1 时，XOR函数返回值为 1；其余情况返回值为 0 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们以“学习 XOR函数”来搭建一个可以完整工作的前馈网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体来说，我们引入一个非常简单的前馈神经网络：\n",
    "\n",
    "* **包含两个单元的隐藏层**：\n",
    "$$\\mathbf{h} = f^{(1)}(\\mathbf{x};\\mathbf{W},c)=g\\circ f  = \\max\\{0, \\mathbf{W}^T\\mathbf{x}+c\\}$$\n",
    "其中 $\\mathbf{x},\\mathbf{W},\\mathbf{c}$ 分别对应着输入、权重矩阵、隐藏层的阈值，$\\mathbf{h} = (h_1, h_2)$ 为隐藏层的输出；激活函数 g 选取了默认推荐的 [线性整流函数(Rectified Linear Unit, ReLU)](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0#%E5%B8%A6%E6%B3%84%E9%9C%B2%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81) \n",
    "\n",
    "$$g(z) = \\max\\{0,z\\}$$\n",
    "\n",
    "* **输出层**：\n",
    "$$y = f^{(2)}(\\mathbf{h};\\mathbf{w},b) = \\mathbf{w}^T\\mathbf{h}+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，我们整个网络的复合映射如下：\n",
    "\n",
    "$$f(\\mathbf{x};\\mathbf{W},\\mathbf{c},\\mathbf{w},b) = \\mathbf{w}^T\\max\\{0,\\mathbf{W}^T\\mathbf{x}+c\\} + b$$\n",
    "\n",
    "下图为对应的拓扑结构计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/XOR.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以给出 XOR 函数的一个解，令\n",
    "\n",
    "$$\\begin{split}\n",
    "\\mathbf{W} &= \n",
    "\\left[                 \n",
    "\\begin{array}{c}   \n",
    "1 & 1\\\\  \n",
    "1 & 1\n",
    "\\end{array} \n",
    "\\right]\\\\\n",
    "\\mathbf{c} &= \n",
    "\\left[                 \n",
    "\\begin{array}{c}   \n",
    "0\\\\  \n",
    "-1\n",
    "\\end{array} \n",
    "\\right]\\\\\n",
    "\\mathbf{w} &= \n",
    "\\left[                 \n",
    "\\begin{array}{c}   \n",
    "1\\\\  \n",
    "-2\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\end{split}$$\n",
    "\n",
    "以及 $b=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入矩阵 $\\mathbf{X}$ 包含二进制输入空间的四个点，每个样本一行，表示如下：\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\left[                 \n",
    "\\begin{array}{c}   \n",
    "0 & 0\\\\  \n",
    "0 & 1\\\\\n",
    "1 & 0\\\\\n",
    "1 & 1\n",
    "\\end{array} \n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带入隐藏层的映射得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{split}\n",
    "\\mathbf{W}^T\\mathbf{x}+c &= \n",
    "\\left[                 \n",
    "\\begin{array}{c}   \n",
    "1 & 1\\\\  \n",
    "1 & 1\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\left[                 \n",
    "\\begin{array}{c}   \n",
    "0 & 0 & 1 & 1\\\\  \n",
    "0 & 1 & 0 & 1\n",
    "\\end{array} \n",
    "\\right] + \n",
    "\\left[                 \n",
    "\\begin{array}{c}   \n",
    "0\\\\  \n",
    "-1\n",
    "\\end{array} \n",
    "\\right]\\\\\n",
    "\\\\\n",
    "&= \\left[                 \n",
    "\\begin{array}{c}   \n",
    "0 & 1 & 1 & 2\\\\  \n",
    "0 & 1 & 1 & 2\n",
    "\\end{array} \n",
    "\\right]+ \n",
    "\\left[                 \n",
    "\\begin{array}{c}   \n",
    "0\\\\  \n",
    "-1\n",
    "\\end{array} \n",
    "\\right]\\\\\n",
    "\\\\\n",
    "&=\\left[                 \n",
    "\\begin{array}{c}   \n",
    "0 & 1 & 1 & 2\\\\  \n",
    "-1 & 0 & 0 & 1\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过激活函数 $max\\{0,z\\}$ 之后，隐藏层的输出为\n",
    "\n",
    "$$\\mathbf{h}=\\left[                 \n",
    "\\begin{array}{c}   \n",
    "0 & 1 & 1 & 2\\\\  \n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则输出层为\n",
    "\n",
    "$$\\begin{split}\n",
    "\\mathbf{w}^T\\mathbf{h}+c &= \n",
    "\\left[                 \n",
    "\\begin{array}{c}   \n",
    "0 &-2\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[                 \n",
    "\\begin{array}{c}   \n",
    "0 & 1 & 1 & 2\\\\  \n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \n",
    "\\right] + 0\\\\\n",
    "\\\\\n",
    "&=\\left[                 \n",
    "\\begin{array}{c}   \n",
    "0 &1 & 1 & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{split}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出恰好是异或函数的解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个例子中，我们简单地指定了解决方案，然后说明它得到的误差为零。实际问题可能会有数十亿的模型参数以及训练样本，不能这样简单猜测。需要使用梯度下降法来求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**基于梯度的学习**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过之前的例子我们可以看到，神经网络和一般的线性模型的最大区别在于，\n",
    "\n",
    "    神经网络中涉及到的非线性(比如激活函数)会导致常用的代价函数非凸。这意味着无法保证神经网络在任意的初始值下最终都全局收敛，而仅仅只能使得代价函数达到一个比较小的值。换言之，用于非凸损失函数的随机梯度下降没有这种收敛性保证，并且对参数初始值很敏感。\n",
    "    \n",
    "    用于训练前馈神经网络以及几乎所有深度模型的迭代求解的梯度类优化算法会在之后详细介绍，包括如何合适地进行参数初始化。\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代价函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度神经网络中一个很重要的环节是代价函数的选择，但实际上神经网络中涉及到的代价函数和机器学习中的代价函数是几乎相同的。\n",
    "\n",
    "大多数时候，神经网络对应的参数模型定义了一个分布 $p(\\mathbf{y}|\\mathbf{x};\\mathbf{\\theta})$，我们可以使用最大似然原理来构建代价函数，由最大似然与最小化交叉熵的等价性，我们可以使用训练数据和模型预测间的交叉熵来作为代价函数。\n",
    "\n",
    "**用于训练神经网络的完整的代价函数，通常会在我们描述的基础代价函数上加一个正则项，用于线性模型的权重衰减方法也适用于深度神经网络，而且是最常用的正则化策略之一。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**使用最大似然学习条件分布**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用最大似然来给出代价函数的优势在于：\n",
    "\n",
    "1. 减轻了为每个模型设计代价函数的负担，明确一个模型 $p(\\mathbf{y}|\\mathbf{x};\\mathbf{\\theta})$ 自动确定了代价函数 $\\log p(\\mathbf{y}|\\mathbf{x};\\mathbf{\\theta})$;\n",
    "\n",
    "\n",
    "2. **很多输出单元都会包含一个指数函数，在变量取绝对值非常大的值时会造成饱和(函数会变得非常平)，这会导致代价函数的梯度变得非常小，负对数似然代价函数中的对数函数抑制了某些输出单元中输出的指数所导致的在错误的样本处饱和不利于梯度下降的特性。详细会在下面的“输出单元”环节介绍。**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始讨论前的四点提示：\n",
    "\n",
    "1. **代价函数的选择与输出单元的数学表达密切相关**，因为代价函数大多数时候要通过梯度求导，所以输出单元的数学表达决定了代价函数的基本样貌，也决定了梯度求导的难易以及是否能够抑制在错误的样本处梯度消失(饱和)的现象；\n",
    "\n",
    "\n",
    "2. 大多时候我们会简单使用数据分布和模型分布之间的**交叉熵(用来衡量两者之间的“距离”，交叉熵越小表示两者更加贴近，等价于最大化似然函数)**，输出的表达形式决定了交叉熵的形式；\n",
    "\n",
    "\n",
    "3. 理论上，下面将要讨论的输出单元的数学表示都适用于神经网络的隐藏单元；\n",
    "\n",
    "\n",
    "4. 在下面的讨论中，我们假设前馈神经网络的隐藏层提供了一组定义为 $\\mathbf{h}=f(\\mathbf{x};\\mathbf{\\theta})$ 的隐藏特征输出作为输出单元的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性单元——用于Gauss 输出分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种基于仿射变换的输出单元我们称之为**线性单元**。 给定输入特征 $\\mathbf{h}$, 线性输出单元层产生一个向量\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{W}^T\\mathbf{h}+\\mathbf{b}$$\n",
    "\n",
    "注：如果矩阵 $\\mathbf{W}^T$ 退化为列向量 $\\mathbf{w}$，输出退化为标量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性输出层经常被用来产生条件高斯分布的期望：\n",
    "\n",
    "$$\\begin{split}p(\\mathbf{y}|\\mathbf{x}) = N(\\mathbf{y};\\hat{\\mathbf{y}},\\mathbf{I}) &= \\sqrt{\\frac{1}{(2\\pi)^n \\det{(I)}}}exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\hat{\\mathbf{y}})^T\\mathbf{I}^{-1}(\\mathbf{y} -\\hat{\\mathbf{y}})\\right)\\\\\n",
    "&=\\sqrt{\\frac{1}{(2\\pi)^n}}exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\hat{\\mathbf{y}})^T(\\mathbf{y} -\\hat{\\mathbf{y}})\\right)\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大化对数似然等价于最小化均方误差：\n",
    "\n",
    "$$\\log(p(\\mathbf{y}|\\mathbf{x})) = \\sqrt{\\frac{1}{(2\\pi)^n}}\\left(-\\frac{1}{2}(\\mathbf{y} - \\hat{\\mathbf{y}})^T(\\mathbf{y} -\\hat{\\mathbf{y}})\\right)\\backsimeq MSE(\\mathbf{y},\\hat{\\mathbf{y}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**关于优化**：因为线性单元不会饱和，所以用梯度优化算法来求解很容易，也可以使用其他很多优化算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "### sigmoid 单元——用于 Bernoulli 输出分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "许多任务需要预测二职型变量 y 的值。下面以具有两个类的分类问题为例来讨论其输出单元的形式。此时我们需要构造 y 在 $\\mathbf{x}$ 条件下的 Bernoulli 分布，然后用最大似然来定义损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli 分布只需要一个参数既可定义，神经网络只需要预测 $P(y=1|\\mathbf{x})$ 既可，当然，它必须满足概率的定义，即\n",
    "\n",
    "$$P(y=1|\\mathbf{x})\\in [0,1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们下面通过 _**输出单元 = 线性计算层 + 激活函数**_ 来定义我们的输出单元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**一个错误的激活函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们通过下面的函数表示来定义我们的输出单元：\n",
    "\n",
    "$$P(y=1|\\mathbf{x}) = \\max\\left\\{0,\\min\\{1, \\mathbf{w}^T\\mathbf{h}+b\\}\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个定义保证了其确实是条件概率分布，满足概率位于 $[0,1]$ 的要求。但当我们使用梯度下降来优化时，我们会发现：当 $\\mathbf{w}^T\\mathbf{h}+b$ 处于单位区间外时，模型的输出对参数的梯度都为 $\\mathbf{0}$。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sigmoid 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样地，我们首先采用一个**线性计算层**来得到\n",
    "\n",
    "$$z = \\mathbf{w}^T\\mathbf{h}+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**激活函数 = 非归一化指数概率分布 + 归一化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "暂时忽略对 $\\mathbf{x}$ 的依赖性，只讨论如何用 z 来得到输出 y=1 的概率分布 $P(y=1|\\mathbf{x})$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定**非对数概率(归一化指数概率分布取对数) $\\hat{P}(y)$ 对可取值 $\\{0,1\\}$ 的随机变量 y 和 z 是线性的**，即\n",
    "\n",
    "$$\\hat{P}(y) = yz$$\n",
    "\n",
    "也就是\n",
    "\n",
    "$$\\hat{P}(y) = e^{yz}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们对其归一化：\n",
    "\n",
    "$$P(y) = \\frac{e^{yz}}{\\sum_{k=\\{0,1\\}}e^{kz}}\\quad y\\in\\{0,1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合我们之前介绍过的Logistic sigmoid 函数\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "我们会发现\n",
    "\n",
    "$$P(y=1) = \\frac{e^z}{1+e^z} = \\sigma(z)\\quad P(y=0) = \\frac{1}{1+e^z}= \\sigma(-z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可统一表达为\n",
    "\n",
    "$$P(y=y_i) = \\sigma\\left((2y_i-1) z\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们得到了随机变量服从 Bernoulli 分布的受到激活函数——sigmoid 变换:\n",
    "\n",
    "$$\\hat{y}=\\sigma(\\mathbf{w}^T\\mathbf{h}+b)$$\n",
    "\n",
    "得到 $\\hat{y}$ 之后再按照实际场景来进行划分类别为 0 和 1 的阈值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代价函数——最大似然估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用最大似然来学习参数是对数空间里预测概率很自然的方法。因为对一个样本 $\\mathbf{x}$ 而言，最大似然对应的最小化代价函数为\n",
    "\n",
    "$$-\\log P(y|\\mathbf{x})$$\n",
    "\n",
    "代价函数中的 log 可以抵消 sigmoid 函数中的 指数 exp，如果没有这种抵消， sigmoid 的饱和性会阻止梯度学习做出好的改进，无法收敛到正确的参数解。我们之后会详细说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过最大似然来学习由 sigmoid 参数化的 Bernoulli 分布，一个样本 $\\mathbf{x}$ 对应的损失函数为\n",
    "\n",
    "$$\\begin{split}\n",
    "J(\\mathbf{\\theta}) &= -\\log P(y|\\mathbf{x})\\\\\n",
    "&=-\\sigma\\left((2y_i-1) z\\right)\\\\\n",
    "&=\\zeta\\left((1-2y_i) z\\right)\n",
    "\\end{split}$$\n",
    "\n",
    "其中 $\\zeta(x)$ 是softplus函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对指数函数饱和性的抑制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由之前我们介绍的[Logistic sigmoid 函数](https://github.com/BofengChen/ml_notes/blob/master/CH0_%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F.ipynb)以及[softplus函数](https://github.com/BofengChen/ml_notes/blob/master/CH0_%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F.ipynb)函数性质可知"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 激活函数 sigmoid 会在 正负无穷远处饱和(见 sigmoid 函数的性质 5 和 6)这种情况下梯度会变得非常小无法通过迭代来更新参数；\n",
    "\n",
    "\n",
    "2. 以对数似然作为损失函数后，根据 softplus 函数的性质，我们会发现 \n",
    "$$(1-2y_i) z \\rightarrow -\\infty\\Rightarrow\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "y_i = 1 &\\quad z\\rightarrow +\\infty\\\\\n",
    "y_i = 0 &\\quad z\\rightarrow -\\infty\\\\\n",
    "\\end{aligned}\n",
    "\\right. $$\n",
    "时，也就是损失函数在正确的样本处(z 的符号与样本对应的 $y_i$ 值一致)才饱和；当 z 的符号与样本对应的 $y_i$ 值不一致时，即\n",
    "$$\\begin{split}\n",
    "\\\\\n",
    "&\\left\\{\\begin{aligned}\n",
    "y_i = 1 &\\quad z\\rightarrow -\\infty\\\\\n",
    "y_i = 0 &\\quad z\\rightarrow +\\infty\\\\\n",
    "\\end{aligned}\\right.\\\\\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "&\\zeta\\left((1-2y_i)z\\right) \\rightarrow |z|\\\\\n",
    "\\Rightarrow &\\frac{\\partial \\zeta}{\\partial z}\\rightarrow sign(z)\n",
    "\\end{split}$$\n",
    "所以对于极限情况下极度不正确的样本， softplus 函数完全不会收缩梯度(会在 sign(z) 两侧振荡)，这意味着基于梯度的参数学习可以很快改正学习错误的样本所对应的 z 值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：理论上 sigmoid 函数的返回值总是被限制在 (0,1) 上，而不是整个单位闭区间。在具体实现时，为了避免数值的上溢或者下溢，**最好将负的对数似然写成 z 的函数而不是 $\\sigma(z)$ 的函数**，因为 sigmoid 函数下溢到零，之后对其取对数就会得到负无穷。\n",
    "\n",
    "具体如何实施呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax 单元——用于 Multinoulli 输出分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当想要表示具有 n 个可能取值的离散型随机变量的分布时，我们可以采用——用于表示二值型随机变量分布的 sigmoid 函数的扩展——**softmax 函数**。\n",
    "\n",
    "下面我们介绍如何构造出适用于构造 Multinoulli 分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**非归一化的对数概率**\n",
    "\n",
    "类似于 Bernoulli 分布，线性层计算得到了未归一化的对数概率：\n",
    "\n",
    "$$\\mathbf{z}=\\mathbf{W}^T\\mathbf{h}+\\mathbf{b}$$\n",
    "\n",
    "其中 \n",
    "\n",
    "$$z_i = \\log\\hat{P}(y=i|\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**归一化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们队 z 指数化和归一化就得到我们所需要的满足概率分布性质(概率向量的每个元素介于 0 和 1 之间，整个向量的和为 1)的 softmax 函数：\n",
    "\n",
    "$$P(y=i;\\mathbf{z}) = \\frac{\\hat{P}(y=i|\\mathbf{x})}{\\sum_j \\hat{P}(y=j|\\mathbf{x})}=\\frac{e^{z_i}}{\\sum_j e^{z_j}}=softmax(\\mathbf{z})_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**损失函数——最大化对数似然**\n",
    "\n",
    "和 上一个章节的 sigmoid 单元一样，我们使用最大似然训练 softmax 来输出目标值 y：\n",
    "\n",
    "$$\\log P(y=i;\\mathbf{z}) = \\log softmax(\\mathbf{z})_i = z_i - \\log\\sum_j e^{z_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对上式最右侧项的理解：\n",
    "\n",
    "1. 第一项 $z_i$ 总是对代价函数有直接的贡献，这一项不会饱和，所以即使 $z_i$ 对第二项的贡献很小，损失函数对参数的学习依然可以进行；\n",
    "\n",
    "\n",
    "2. 最大化对数似然时，**第一项鼓励 $z_i$ 被推高，第二项鼓励所有的 $\\mathbf{z}$ 被压低**；\n",
    "\n",
    "\n",
    "3. 对第二项的直观理解，我们先改变其表达形式如下：\n",
    "$$\\begin{split}\n",
    "\\\\\n",
    "z_i - \\log\\sum_j e^{z_j} &= z_i - \\log exp(\\max_k z_k)\\sum_j exp(z_j - \\max_k z_k)\\\\\n",
    "&=z_i - \\log exp(\\max_k z_k)\\sum_j exp(z_j - \\max_k z_k)\\\\\n",
    "&=z_i - \\max_k z_k - \\log\\left(1+\\sum_{j\\neq k} exp(z_j - \\max_k z_k)\\right)\\\\\n",
    "&(if \\quad z_j \\ll \\max_k z_k\\;(j\\neq k))\\\\\n",
    "&=z_i - \\max_k z_k\n",
    "\\end{split}$$\n",
    "从最后的结果来看：\n",
    "    * 负对数似然总是严重惩罚最活跃的不正确预测；\n",
    "    * 正确的预测恰好是最大输入，即 $max_k z_k = z_j$，则上式大致抵消为 0，这个样本对整体训练代价贡献很小；这个代价主要由其他未被正确分类的样本产生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**最大似然与 softmax 函数的契合**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似于 sigmoid 函数， [softmax 函数](https://github.com/BofengChen/ml_notes/blob/master/CH0_%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F.ipynb)的每一项都有可能饱和于 0 和 1（当该项趋于无穷大或者无穷小的时候）。所以一般的目标函数对 softmax 函数不起作用。不使用对数来抵消 softmax 中指数的目标函数，当**指数函数的变量取非常小的负值时**会造成梯度消失。\n",
    "\n",
    "尤其是**平方误差**，对 softmax 函数来说，是一个很差的损失函数，即使模型做出明显的不正确预测，也不能通过损失函数训练模型来改变和纠正其输出。\n",
    "\n",
    "注：这里的**“变量取非常小的负值”**应该含义是，变量的大部分分量都趋向于负无穷时，会导致 $\\mathbf{z}$ 的 n-1 个分量都饱和到 0，其中对应变量 $\\mathbf{x}$ 最大值的 $\\mathbf{z}$ 的分量饱和到1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**神经科学的角度**\n",
    "\n",
    "从神经科学角度看，认为 softmax 是一种在参与其中的单元之间形成竞争的方式。softmax 输出总和为 1，所以一个单元的值增加必然对应其他单元值的减少。这与被认为存在于皮质中相邻神经元之间的侧抑制类似。极端情况下，（当最大的分量和天气在幅度上差异很大时），它变成了**赢者通吃(winner-take-all)**的形式（其中一个输出接近1，其他接近0）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隐藏单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们转向前馈神经网络的隐藏层中的元素——隐藏单元的设计，这是一个很活跃的研究领域，但是还没有明确的指导性理论原则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**隐藏单元的选择**\n",
    "\n",
    "决定使用哪种类型的隐藏单元是很困难的，一般是基于直觉，然后通过神经网路进行训练以及验证集来评估性能。ReLU 是隐藏单元极好的默认选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐藏单元的不可微讨论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们列出的一些隐藏单元并不是在所有的输入点处都可微分。例如 $g(z)=\\max(0,z)$ 在 0 点是不可微的，但实践中并不影响基于梯度下降来学习参数，而且表现还相当好。原因在于：**神经网络训练算法通常不会达到代价函数的局部最小值，而是仅仅显著减小它的值。**如下图所示。\n",
    "\n",
    "因为我们不在期望训练能够真正到达梯度为 $\\mathbf{0}$ 的点，所以代价函数最小值恰好对应于梯度函数未定义的点是可以接受的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/localmin.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来讲，我们选择的隐藏单元，其不可微的点通常只在少数点上不可微。**对于不可微的点，如果情况是左导数和右导数都存在，但是不相等。一般计算导数的时候会返回其中一个作为导数的代替。**\n",
    "\n",
    "这个和在计算机上基于梯度的优化总是会受到数值误差的影响，某种程度上是类似的问题。当一个函数要求计算 $g(0)$ 时，底层值真正为 0 是不太可能的，而可能是一个被舍入为 0 的一个很小的量 $\\epsilon$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**隐藏单元的一般模式**\n",
    "\n",
    "除非特别说明，大多数隐藏单元都可以被描述为 **隐藏单元 = 线性层 + 激活函数**，即\n",
    "\n",
    "$$\\mathbf{h} = g(\\mathbf{z})$$\n",
    "\n",
    "其中 $\\mathbf{x}$ 是输入向量， $\\mathbf{z} = \\mathbf{W}^T\\mathbf{x}+\\mathbf{b}$ 是仿射变换， g 是逐元素也就是采用广播机制的非线性函数。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU 及其扩展——近似线性的隐藏单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU 使用激活函数 $g(z)=\\max\\{0,z\\}$, 和线性单元很类似。而且只要 ReLU 处于激活状态，导数都保持的比较大且一致，也就是 1。\n",
    "\n",
    "ReLU 通常作用于仿射变换之上：\n",
    "\n",
    "$$\\mathbf{h}=g(\\mathbf{W}^T\\mathbf{x}+\\mathbf{b})$$\n",
    "\n",
    "初始化仿射变换的参数时，可以将 $\\mathbf{b}$ 的所有元素设置成一个很小的元素，例如 0.1（输入 $\\mathbf{x}$ 设置为 $\\mathbf{0}$ 么？），这使得 ReLU 在初始的时候对训练集的大多数输入呈现激活状态，并且允许导数通过。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "ReLU 和其扩展都是基于一个原则——**“如果它们的行为更接近线性，模型更容易优化。”**\n",
    "\n",
    "这个一般性原则不仅适用于本节介绍的深度前馈神经网络，也适用于循环网络等其他场景。比如 LSTM，作为性能最好的循环网络结构之一， LSTM 通过求和在时间维度上传播信息，这是一种特别直观的线性激活，我们会在之后进行详细的讨论。\n",
    "***\n",
    "**ReLU 比很多激活函数作为隐藏单元性能好**的原因在于它描述了生物神经元的如下性质：\n",
    "\n",
    "* 对于某些输入，生物神经元是完全不活跃的；\n",
    "\n",
    "* 对于某些输入，生物神经元的输出和它的输入成正比例；\n",
    "\n",
    "* 大多数时间，生物神经元是在它们不活跃的状态下进行运转的（即它们应该具有**稀疏激活(sparse activation)**性质）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 弥补梯度的扩展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU 的一个缺陷在于无法通过基于梯度的方法学习那些使 ReLU 为 0 的样本：\n",
    "\n",
    "$$g(z) = 0 \\quad if \\quad \\mathbf{W}^T\\mathbf{x}_i+\\mathbf{b}\\leq 0$$ \n",
    "\n",
    "ReLU 的扩展可以保证激活函数可以在各个位置都接收到梯度。性能上讲，大多数性能和 ReLU 差不多，偶尔表现地更好（Why?）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU 扩展的一般形式为:\n",
    "\n",
    "$$\\mathbf{h} = g(\\mathbf{z},\\mathbf{\\alpha})$$\n",
    "\n",
    "其中 \n",
    "\n",
    "$$\\begin{split}\n",
    "h_i = g(\\mathbf{z},\\mathbf{\\alpha})_i &= \\max(0,z_i)+\\alpha_i\\min(0,z_i)\\\\\n",
    "&=\\left\\{\\begin{aligned}\n",
    "&z_i  &z_i>0\\\\\n",
    "&\\alpha_i z_i &z_i\\leq 0\n",
    "\\end{aligned}\n",
    "\\right.\\end{split}$$\n",
    "\n",
    "其中 $\\alpha_i \\neq 0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变 $\\alpha_i$ 的状态，我们可以得到 ReLU 的不同扩展：\n",
    "\n",
    "\n",
    "* 当 $\\alpha_i=-1$ 时，我们有\n",
    "$$g(z)=|z|$$\n",
    "我们称这种扩展为**绝对值整流**；\n",
    "\n",
    "\n",
    "* 当 $\\alpha_i$ 固定一个类似于 0.01 的小的数值时，我们称这种扩展为**渗漏整流线性单元**，这个时候激活函数的左半部分是一条贴近于 $z_i=0$ 的直线；\n",
    "\n",
    "\n",
    "* 当 $\\alpha_i$ 被视为学习的参数时，称之为 **参数化整流线性单元或 PReLU**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [maxout 单元](https://arxiv.org/pdf/1302.4389.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxout 单元和上面的扩展方式不同，它某种意义上改变了从第 i 层到 第 i+1 层的网络结构，在中间加了一个基于隐层的隐层，我们可以称之为“隐隐层”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设该隐层的输入是 $\\mathbf{x}\\in R^d$，我们定义激活函数为\n",
    "\n",
    "$$h_i(\\mathbf{x}) = \\max_{j\\in[1,k]}z_{ij}$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$z_{ij}=\\mathbf{x}^T\\mathbf{W}_{:,ij}+b_{ij}\\quad where \\; \\mathbf{W}\\in R^{d\\times m\\times k},\\mathbf{b}\\in  R^{m\\times k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示意图如下：\n",
    "\n",
    "<img src='figure/maxout.jpg' width='400' height='300' align='left'>\n",
    "<img src='figure/maxout1.png' width='400' height='300' align='right'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来简单对比下 ReLU 和 maxout 单元。\n",
    "\n",
    "假设输入层 InputL 有 d 个神经元，隐层 HL1 有 m 个单元，按照激活函数的不同：\n",
    "\n",
    "* 如果激活函数为 ReLU,从 InputL 到 HL1 的权重矩阵为\n",
    "\n",
    "$$InputL\\rightarrow HL1:\\; \\mathbf{W}_1\\in R^{d\\times m}, \\mathbf{b}_1 \\in R^{m\\times 1}$$\n",
    "\n",
    "\n",
    "* 如果激活函数为 maxout 单元, 记介于 InputL 和 HL1 之间的隐隐层为 HHL1，从 InputL 到 HHL1 的权重矩阵为\n",
    "\n",
    "$$InputL\\rightarrow HHL1:\\; \\mathbf{W}_2\\in R^{d\\times m\\times k}, \\mathbf{b}_2 \\in R^{m\\times k}$$\n",
    "\n",
    "* 针对 HL1 层的第 $i\\; (1\\leq i\\leq m)$ 个单元，HHL1 层对应的 k 个神经元为 $HHL1_{ij},\\; j\\in[1,k]$，则 InputL 到 $HHL1_{ij}$ 的切片权重矩阵为\n",
    "\n",
    "$$InputL\\rightarrow HHL1_{ij}:\\; \\mathbf{W}_2^{sub}\\in R^{d\\times k}, \\mathbf{b}_2^{sub}\\in R^{1\\times k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxut 单元可以学习多达 k 段的分段线性的凸函数，因此 maxout 单元被视为**学习激活函数本身，而不仅仅是单元之间的关系**。\n",
    "\n",
    "**使用足够大的 k, maxout 单元可以以任意的精度近似任何凸函数。**特别地，当 $k=2$ 时，maxout 层可以学习和实现和传统层相同的激活函数，包括 ReLU以及其扩展，以及与这些都不同的激活函数。当然， maxout 层的参数化与其他传统的激活函数的学习原理也是不同的（具体如何不同？）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**正则化**\n",
    "\n",
    "maxout 单元现在由 k 个权重向量来参数化，而不是一个，相当于一个切片的权重矩阵，所以 maxout 单元通常被 ReLU 需要更多的正则化来防止过拟合（更多是什么意思？？）。如果训练集很大并且 k 的数值保持很小的话，可以在没有正则化的情况下很好地拟合。\n",
    "\n",
    "**灾难遗忘**\n",
    "\n",
    "**灾难遗忘现象**是指 当训练好的神经网络经过一批新的数据训练后，会无法很好识别原来的数据，也就是会遗忘原来的学习任务。\n",
    "\n",
    "maxout 层的每个单元由多个过滤器驱动，即切片权重矩阵。maxout 单元有一些冗余来抵抗灾难遗忘现象。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tanh 激活函数——比 sigmoid 更适合的隐藏单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 ReLU 出现之前，大多数神经网络都使用 sigmoid 函数或者 [双曲正切函数 tanh](https://github.com/BofengChen/ml_notes/blob/master/CH0_%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F.ipynb) 来作为激活函数。\n",
    "\n",
    "双曲正切函数定义如下：\n",
    "\n",
    "$$g(z)=\\tanh(z) = \\frac{\\sinh(z)}{\\cosh(z)}=\\frac{e^z - e^{-z}}{e^z + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh 函数与 sigmoid 密切相关，因为 $\\tanh(z)=2\\sigma(2z)-1$，两个函数有很多很类似的性质。从这个表达式也可以认为**tanh 函数是 sigmoid 函数通过伸缩平移得到的函数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为 sigmoid 函数的高度饱和性，所以并不建议将它作为前馈网络中的隐层单元，更适合做输出单元的激活函数，可以和基于梯度的学习兼容。\n",
    "\n",
    "作为隐藏单元的激活函数，tanh 函数往往要比 sigmoid 函数表现更好。在 0 点附近，\n",
    "\n",
    "$$\\tanh(0)=0 \\; \\sigma(0)=\\frac{1}{2}$$\n",
    "\n",
    "或者更直接地，观察tanh 函数的泰勒展开，会发现\n",
    "\n",
    "$$\\tanh(x)\\sim x ,x\\rightarrow 0$$\n",
    "\n",
    "所以在 0 附近， tanh 函数与 $y=x$ 近似。\n",
    "\n",
    "因为这个良好的性质，在训练深层神经网络\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}^T\\tanh\\left(\\mathbf{U}^T\\tanh(\\mathbf{V}^T\\mathbf{x})\\right)$$\n",
    "\n",
    "时，类似于训练一个线性模型\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}^T\\mathbf{U}^T\\mathbf{V}^T\\mathbf{x}$$\n",
    "\n",
    "只要网络的激活函数能够被保持地足够小（因为只有在零点附近，才会有这种线性近似。可是如何保持足够小呢？？），会使得训练 tanh 网络很容易。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他隐藏单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们列出一些特别有用和独特的隐藏单元进行强调。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**单位函数 $y=x$ 做激活函数**\n",
    "\n",
    "这是一种完全没有激活函数 $g(z)=z$，也可以认为是单位函数作为激活函数。\n",
    "\n",
    "之前我们看到过线性单元可以作为输出单元，也可以用作隐藏单元。神经网络的一些层是纯线性也是可以接受的。考虑具有 n 个输入和 p 个输出的神经网络层\n",
    "\n",
    "$$\\mathbf{h}=g(\\mathbf{W}^T\\mathbf{x}+ \\mathbf{b})$$\n",
    "\n",
    "可以用两层来代替它，两层分别使用权重矩阵 $\\mathbf{U},\\mathbf{V}$,如果第一层没有激活函数，我们可以对基于 $\\mathbf{W}$ 的原始层的权重矩阵进行因式分解，分解方法是计算\n",
    "\n",
    "$$\\mathbf{h}=g(\\mathbf{V}^T\\mathbf{U}^T\\mathbf{x}+ \\mathbf{b})$$\n",
    "\n",
    "如果 $\\mathbf{U}$ 产生了 q 个输出，那么 $\\mathbf{U},\\mathbf{V}$ 一起包含 $(n+p)q$ 个参数，$\\mathbf{W}$ 包含 np 个参数。如果 q 很小使得\n",
    "\n",
    "$$(n+p)q \\ll np$$\n",
    "\n",
    "则很大程度上可以节约参数。\n",
    "\n",
    "这是将线性变换约束为低秩的代价来实现的，但低秩很多时候是足够的。**线性隐藏单元因此提供了一种减少网络中参数数量的有效方法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**softmax 单元**\n",
    "\n",
    "softmax 单元有时也可以用作隐藏单元。softmax 单元很自然表示 k 个可能值的离散型随机变量的概率分布，所以可以用作一种开关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**其他一些常见的隐藏单元**：\n",
    "\n",
    "1. **径向基函数(RBF)**: $h_i =exp\\left(-\\frac{1}{\\sigma_i}||\\mathbf{W}_{:,i}-\\mathbf{x}||^2\\right)$。如下左图所示，这个函数在 $\\mathbf{x}$ 靠近 $\\mathbf{W}_{:,i}$ 更加活跃，在趋向正负无穷时会饱和到 0。\n",
    "\n",
    "\n",
    "2. **softplus 函数**： $g(x)=\\zeta(x)=\\log(1+e^x)$ 是 ReLU 的平滑版本。根据实际经验发现，**softplus 作为隐藏单元的性能是比 ReLU 差的，这个说明隐藏单元的性能并不一定和其数学性质的优劣成正比，并不是处处可导或者完全不饱和，对应的隐藏单元性能一定就好。**所以从这个意义上讲，深度学习还没有找到属于自己的一套理论的支撑。\n",
    "\n",
    "\n",
    "3. **硬双曲正切函数(hard tanh)**：其定义为\n",
    "$$\\begin{split}\n",
    "g(x)&=\\max(-1,\\min(1,x)) \\\\\n",
    "&=\\left\\{\\begin{aligned}\n",
    "&1  & x\\geq 1\\\\\n",
    "&x &-1\\leq x< 1\\\\\n",
    "&-1 & x\\leq -1\n",
    "\\end{aligned}\n",
    "\\right.\\end{split}$$\n",
    "如下图右侧图所示，形状和 tanh、ReLU 类似，但是不同于 ReLU,它是有界的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/gauss.jpg' width='400' height='300' align='left'>\n",
    "<img src='figure/hard_tanh.png' width='400' height='300' align='right'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 架构设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的另一个关键点是确定其**架构**：应该具有多少单元，以及这些单元如何连接。\n",
    "\n",
    "大多数神经网络以**“层”**为整个网络的单位模块，通过**链式结构**将层衔接起来，其中每一层都是前一层的函数：\n",
    "\n",
    "$$\\mathbf{h}^{(i)} = g^{(i)}\\left(\\mathbf{W}^{(i)T}\\mathbf{h}^{(i-1)}+\\mathbf{b}^{(i)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在链式结构中，主要的架构考虑的是选择网络的深度和每一层的宽度**：\n",
    "\n",
    "1. 即使只有一个隐藏层的网络也足够适应训练集；\n",
    "2. 更深层的网络通常能够使得每一层的模块使用更少的神经单元和更少的参数，更容易泛化到测试集，但也更难优化。\n",
    "\n",
    "对于具体的任务，需要通过实验，观测验证集上的误差来寻找最适合的网络结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 万能近似性质和深度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**万能近似定理**是指：\n",
    "\n",
    "    一个前馈神经网络如果具有线性输出层和至少一层具有任何“挤压”性质的激活函数(例如 logistic sigmoid 函数)并且足够多数量隐藏单元的隐藏层，可以以任意精度近似任何一个从有限维空间到另一个有限维空间的 Borel 可测函数。前馈网络的导数可以任意近似函数的导数。\n",
    "    \n",
    "Borel 可测的概念可以参考 [Borel 可测函数](https://zh.wikipedia.org/wiki/%E5%8D%9A%E9%9B%B7%E5%B0%94%E5%8F%AF%E6%B5%8B%E5%87%BD%E6%95%B0)，定义在 $R^n$ 上有界闭集上的任意连续函数都是 Borel 可测的，因此可以用神经网络来近似。神经网络也可以近似从任何有限维离散空间映射到另一个有限维离散空间的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "万能近似定理告诉我们可以学习表示任何函数的足够大的MLP的存在性，但我们无法保证训练算法能够学习到这个函数，也就是**构造性的通道**无法保证。原因如下：\n",
    "\n",
    "1. 用于训练的优化算法可能找不到用于期望函数的参数值；\n",
    "2. 训练算法可能由于过拟合选择了错误的函数。\n",
    "\n",
    "想想我们前面介绍的“没有免费午餐”定理，没有普遍优越的机器学习算法。前馈网络提供了表示函数的万能系统，但不存在**万能的过程既能够验证训练集上的样本，又能够恰好选择一个函数将其扩展到训练集上没有涉及的点。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**为何选择深度神经网络而不是浅层**\n",
    "\n",
    "* 具有单层的前馈网络足以表示任何函数，但是网络层可能大到无法实现，并且可能无法正确学习和泛化。很多情况下，使用更深的模型能够减少期望函数所需的单元的数量，并且可以减少泛化误差。\n",
    "  \n",
    "  Montufar 指出一些用深度 ReLU网络表示的函数可能需要浅层网络(一个隐藏层)指数级的隐藏单元才能表示。\n",
    "  \n",
    "  \n",
    " * 我们还会基于统计的原因来选择深度模型。选择深度模型是基于一个非常普遍的信念：我们想要学习的函数是几个更加简单函数的组合。从表示学习的观点来解释，我们相信学习问题等价于发现一组潜在的变差(variation)因素，而这些因素又可以根据其他更简单的潜在变差因素来描述。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他架构上的考虑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们之前都将神经网络描述成以层为模块的简单链式结构，确定这个模式后我们只考虑层的数量以及每层的宽度即隐藏神经元的数量。实践中，神经网络不只是链式结构。\n",
    "\n",
    "另外两个设计上的可能性在于：\n",
    "\n",
    "1. **在非相邻层之间建立连接**：比如从层 i 到 层 i+2 或者更高层的跳跃连接。这些跳跃使得梯度更容易从输出层流向更接近输入的层；\n",
    "\n",
    "\n",
    "2. **相邻层之间的神经元进行部分连接**：默认的神经网络层采用矩阵 $\\mathbf{W}$ 描述的线性变换，连接任意的输入单元到每个下一层的输出单元。\n",
    "\n",
    "    实际操作中，允许输入层的每个单元仅连接到输出层的一个小子集。通常用于减少连接数量的策略减少了参数的数量以及优化网络的计算量，但通常高度依赖于实际问题。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP反向传播算法及其他微分算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于前向传播和反向传播：\n",
    "\n",
    "1. **前向传播**：前馈神经网络接收输入 $\\mathbf{x}$ ，然后传播到每一层的隐藏单元，最终产生输出 $\\hat{\\mathbf{y}}$,称之为前向传播；\n",
    "2. **后向传播**：在训练过程中，前向传播可以持续往前直到产生一个标量代价函数 $J(\\mathbf{\\theta})$。**后向传播(back propagation)算法**，经常被称为 **backprop**， 允许来自代价函数的信息通过网络向后流动，以便计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**数值化求解梯度的反向传播**\n",
    "\n",
    "1. **计算梯度**：求解一个函数的梯度，解析表达式是很直观的，但是在有限的时间和有限的计算资源和有限的存储下数值化求解这样的表达式在计算上的代价可能会很大。反向传播使用简单成本低的程序来实现对梯度的计算。\n",
    "\n",
    "\n",
    "2. **对反向传播的误解**：\n",
    "\n",
    "    * 反向传播术语经常被误解为用于多层神经网络的整个学习算法，其实仅仅指**用于计算梯度的方法**。而比如随机梯度下降这个优化算法，使用反向传播计算得来的梯度来进行学习；\n",
    "    \n",
    "    * 反向传播还被误解为仅适用于多层神经网络，但是原则上它可以用于计算任何函数的导数(如果函数局部不可微，可以重新定义导数)。一般意义上讲，反向传播可以用来计算 $\\nabla_{\\mathbf{x}}f(\\mathbf{x},\\mathbf{y})$，其中 $\\mathbf{x}$ 可以视为需要求解的参数变量，$\\mathbf{y}$ 是一组不需要求解梯度的输入变量。\n",
    "***   \n",
    "在学习算法中，我们经常需要的梯度是代价函数关于参数的梯度，即 $\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$。反向传播也适用于机器学习中对导数的计算，不局限于计算代价函数的梯度，还可以用于计算输出函数 f 的 Jacobian 值。我们这里描述的是最常用的情形，其中 f 只是单个输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**微积分中的链式法则**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设 $\\mathbf{x}\\in R^{m},\\mathbf{y}\\in R^{n}$， g 是从 $R^{m}$ 到 $R^{n}$ 的映射， f 是从 $R^{n}$ 到 $R$ 的映射。如果 $\\mathbf{y}=g(\\mathbf{x})$ 并且 $\\mathbf{z}=f(\\mathbf{y})$，那么\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x_i}=\\sum_{j}\\frac{\\partial z}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用向量记法，可以等价写成\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}}z=(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}})^T\\nabla_{\\mathbf{y}}z$$\n",
    "\n",
    "这里 $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ 是关于 g 的 $n\\times m$的 Jacobian 矩阵。\n",
    "\n",
    "反向传播算法就是图中每一个这样的 Jacobian 矩阵与梯度的乘积组成的。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 递归使用链式法则来实现反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "_**莎士比亚难题：to be or not to be**_\n",
    "\n",
    "使用上面的链式法则可以直接写出某个标量关于计算图中任何产生该标量的节点其对应的袋鼠表达式，但实际在计算机中我们要考虑如下问题：许多子表达式在梯度的计算过程中会重复若干次。基于这个现象，计算梯度时需要考虑**存储这些子表达式还是在相应环节的时候重新计算它**：\n",
    "\n",
    "* 复杂图中计算梯度时，可能存在指数级别这种计算上的浪费；\n",
    "\n",
    "* 另一方面，当内存受限时，可以通过计算多次相同的子表达式是以运行时间代价来换取内存开销。\n",
    "\n",
    "下图给出了一个例子来说明这些重复的子表达式是如何出现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/repeated_subexpressions.jpg' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "下面我们给出简化的反向传播算法。它指明了梯度的直接计算方式。我们可以**直接执行这些计算**或者将算法描述视为**用于计算反向传播的计算图的符号表示**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们要解决的问题是：**计算标量 $u^{(n)}$(对应单个或者小批量示例的代价函数)对 $n_i$ 个模型的参数(即计算图中 $n_i$ 输入节点) $u^{(1)}$ 到 $u^{(n_i)}$ 的梯度：\n",
    "\n",
    "$$\\frac{\\partial u^{(n)}}{\\partial u^{(i)}}$$\n",
    "\n",
    "其中 $i\\in \\{1,2,\\cdots,n_i\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    " _**前向传播**_\n",
    "\n",
    "我们先给出前向传播的算法伪代码。假设图的节点已经以一种特殊的方式被排序，使得我们可以按照从 $u^{(n_i+1)}$ 到 $u^{(n)}$ 一步步计算其输出。\n",
    "\n",
    "**算法1.1**\n",
    "\n",
    "**目标**：计算将 $n_i$ 个参数输入 $\\{u^{(1)},\\cdots,u^{(n_i)}\\}$ 映射到 $u^{(n)}$ 的程序；\n",
    "\n",
    "**输入**：样例向量 $\\mathbf{x}$,被分配给前 $n_i$个节点 $\\{u^{(1)},\\cdots,u^{(n_i)}\\}$；\n",
    "\n",
    "**输出**：最后一个节点 $u^{(n)}$;\n",
    "\n",
    "\n",
    "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "\n",
    "**for** $i=1,2,\\cdots,n_i$ **do**\n",
    "\n",
    "$u^{(i)}\\leftarrow x_i$\n",
    "\n",
    "**end for**\n",
    "\n",
    "**for** $i=n_i+1\\cdots,n$ **do**\n",
    "\n",
    "$$\\begin{split}\n",
    "&A^{(i)}\\leftarrow \\{u^{(j)}|j\\in Pa(u^{(i)}),j< i\\}\\\\\n",
    "&u^{(i)}\\leftarrow f^{(i)}(A^{(i)})\\end{split}$$\n",
    "\n",
    "**end for**\n",
    "\n",
    "**return** $u^{(i)}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "其中 $f^{(i)}$ 是对应到节点 $u^{(i)}$ 的计算图操作；$ Pa(u^{(i)})$ 表示 $u^{(i)}$ 的父节点。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _**反向传播算法(简化版本)**_\n",
    " \n",
    " 这个示例旨在通过演示所有变量都是标量的简化情况下进一步理解反向传播算法。\n",
    "\n",
    "对于 $u^{(i)}$ 而言，$\\frac{\\partial u^{(i)}}{\\partial u^{(j)}}$ 是关于父节点 $u^{(j)}$ 的函数，从而可以将前向图中的节点链接到反向传播图中添加的节点。\n",
    "\n",
    "\n",
    "**算法1.2**\n",
    "\n",
    "**目标**：计算标量 $u^{(n)}$ 对 $n_i$ 个模型的参数 $u^{(1)}$ 到 $u^{(n_i)}$ 的梯度 $\\frac{\\partial u^{(n)}}{\\partial u^{(i)}},i\\in \\{1,2,\\cdots,n_i\\}$。\n",
    "\n",
    "**输入**：\n",
    "* 运行前向传播对应的算法1.1,将网络激活；\n",
    "* 初始化用于存储计算好的导数的数据结构 grad_table，$grad\\_table[u^{(i)}]$ 将存储 $\\frac{\\partial u^{(n)}}{\\partial u^{(i)}}$\n",
    "。\n",
    "\n",
    "**输出**：$\\frac{\\partial u^{(n)}}{\\partial u^{(i)}},i\\in \\{1,2,\\cdots,n_i\\}$。\n",
    "\n",
    "\n",
    "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "$grad\\_table[u^{(n)}]\\leftarrow 1$\n",
    "\n",
    "**for** j=n-1 down to 1 **do**\n",
    "\n",
    "$$grad\\_table[u^{(j)}]\\leftarrow \\sum_{i\\in B_i}grad\\_table[u^{(i)}]\\frac{\\partial u^{(i)}}{\\partial u^{(j)}}$$\n",
    "\n",
    "其中 $B_i = \\{i|i\\in Son(u^{(j)})\\}$，$Son(u^{(j)})$ 是节点 $u^{(j)}$ 的所有子节点。\n",
    "\n",
    "上式的梯度计算实际等价于如下的计算：\n",
    "\n",
    "$$\\frac{\\partial u^{(n)}}{\\partial u^{(j)}}=\\sum_{i\\in B_i}\\frac{\\partial u^{(n)}}{\\partial u^{(i)}}\\frac{\\partial u^{(i)}}{\\partial u^{(j)}}$$\n",
    "\n",
    "**end for**\n",
    "\n",
    "**return** $grad\\_table[u^{(i)}],i\\in \\{1,2,\\cdots,n_i\\}$\n",
    "\n",
    "\n",
    "\n",
    "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "反向传播算法被设计成减少重复的公共子表达式的数量而不考虑存储开销。具体来说，它大约对图中的每个节点执行一次 Jacobian 矩阵与梯度的乘积。其中$\\frac{\\partial u^{(i)}}{\\partial u^{(j)}}$ 是 Jacobian 矩阵的元素 $a_{ij}$，$\\frac{\\partial u^{(n)}}{\\partial u^{(i)}}$ 对应的是梯度。\n",
    "\n",
    "注：从计算图的结构角度考虑，实际上会出现 $\\{u^{(j)},u^{(j+1)},\\cdots, u^{(j+k)}\\}$ 同属于一个计算图的同一个级别的情况，所以“j=n-1 down to 1”中并不是相邻节点都是父子节点的关系。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 符号到符号的导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代数表达式和计算图都对 **符号(symbol)** 或不具有特定值的变量进行操作。这些代数或者基于图的表达式被称为**符号表示(symbol representation)**。\n",
    "\n",
    "两种思路：\n",
    "\n",
    "1. 一些反向传播的方法采用计算图和一组用于图的输入的数值，然后返回这些输入值处梯度的一组数值，这种方法称为**符号到数值**的微分，比如 Torch 和 Caffe 之类的库中都是这种方式；\n",
    "\n",
    "\n",
    "2. 另外一些方法是采用计算图以及添加一些额外的节点到计算图中，这些额外的节点提供了我们所需导数的符号描述。这是 Theano 和 Tensorflow 所采用的的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图给出了思路2所对应的“符号到符号”如何工作的一个例子。\n",
    "\n",
    "<img src='figure/symbol-to-symbol.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单个隐藏层的MLP 代价函数计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记 MLP 对应的模型为\n",
    "\n",
    "$$\\mathbf{y} = max\\{0,\\mathbf{X}\\mathbf{W}^{(1)}\\}\\mathbf{W}^{(2)}$$\n",
    "\n",
    "包含了正则项+交叉熵的代价函数：\n",
    "\n",
    "$$J = J_{MLE}+ \\lambda\\left(\\sum_{i,j}\\left(\\mathbf{W}^{(1)}_{i,j}\\right)^2 + \\left(\\mathbf{W}^{(2)}_{i,j}\\right)^2 \\right)$$\n",
    "\n",
    "对应的计算图如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/back-propagation-mlp.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习以外的微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**自动微分(automatic differentiation)** 领域关心如何以算法方式计算导数，上面我们描述的反向传播算法只是自动微分的一种方法。它是一种称为**反向模式累加(reverse mode accumulation)**这种更广泛方法的特殊情况。\n",
    "\n",
    "当图的输出数目大于输入数目时，有时更偏向于使用另外一种形式的自动微分，称为**前向模式累加(forward mode accumulation)**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向模式和后向模式的关系类似于左乘和右乘一系列矩阵之间的关系。例如\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{B}\\mathbf{C}\\mathbf{D}$$\n",
    "\n",
    "其中的矩阵可以认为是 Jacobian 矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种模式的讨论：\n",
    "\n",
    "* 如果 $\\mathbf{D}$ 是列向量，$\\mathbf{A}$ 有很多行，对应于一幅有很多输入和单个输出的图，并且从最后开始乘，反向进行，只需要 **矩阵-向量** 的乘积，对应反向模式；\n",
    "\n",
    "* 从左边开始乘涉及一系列的 **矩阵-矩阵** 乘积，这使得总的计算成本更高。当然，如果 $\\mathbf{A}$ 的行数小于 $\\mathbf{D}$ 的列数，则从左到右乘的计算成本更低。\n",
    "\n",
    "经过上面的讨论，我们知道反向传播不是计算梯度的唯一方式或者最佳方式，但非常实用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "356px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
